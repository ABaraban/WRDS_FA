{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import wrds\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "# import ray\n",
    "# import os\n",
    "\n",
    "# # Set environment variables for Modin and Ray\n",
    "# os.environ[\"MODIN_ENGINE\"] = \"ray\"\n",
    "# os.environ[\"__MODIN_AUTOIMPORT_PANDAS__\"] = \"1\"\n",
    "# # Look at the Ray documentation with respect to the Ray configuration suited to you most.\n",
    "# ray.init()\n",
    "\n",
    "def fetch_prices_for_dates(cusip_list, date_list):\n",
    "    print(\"Fetching\")\n",
    "    cusip_list_str = \"', '\".join(cusip_list)\n",
    "    date_list_str = \"', '\".join(pd.to_datetime(date_list).strftime('%Y-%m-%d'))\n",
    "    \n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            cusip,\n",
    "            datadate,\n",
    "            ajexdi,\n",
    "            prccd,\n",
    "            trfd\n",
    "        FROM \n",
    "            comp_na_daily_all.secd\n",
    "        WHERE \n",
    "            cusip IN ('{cusip_list_str}') AND\n",
    "            datadate IN ('{date_list_str}')\n",
    "    \"\"\"\n",
    "    return db.raw_sql(query)\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "        \n",
    "def first_non_na(row, columns):\n",
    "    for col in columns:\n",
    "        if pd.notna(row[col]):\n",
    "            return row[col]\n",
    "    return None\n",
    "\n",
    "def downcast_numeric_columns(df):\n",
    "    \"\"\"\n",
    "    Downcast numerical columns in a pandas DataFrame to reduce memory usage.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame to downcast.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with downcasted numeric columns.\n",
    "    \"\"\"\n",
    "    # Downcast integer columns\n",
    "    int_cols = df.select_dtypes(include=['int', 'int64']).columns\n",
    "    df[int_cols] = df[int_cols].apply(pd.to_numeric, downcast='integer')\n",
    "    \n",
    "    # Downcast float columns\n",
    "    float_cols = df.select_dtypes(include=['float', 'float64']).columns\n",
    "    df[float_cols] = df[float_cols].apply(pd.to_numeric, downcast='float')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Establish a connection to the WRDS database\n",
    "db = wrds.Connection(wrds_username='asherbaraban')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insiders_df = pd.read_pickle(\"raw_insiders_pre_merge.pkl\")\n",
    "# cusip_list = insiders_df['cusipi'].unique().tolist()\n",
    "# trandate_list = insiders_df['trandate'].unique().tolist()\n",
    "\n",
    "# # Generate the additional dates\n",
    "# trandate_6mo_list = (insiders_df['trandate'] + pd.DateOffset(months=6)).unique().tolist()\n",
    "# trandate_6mo_1d_list = (insiders_df['trandate'] + pd.DateOffset(months=6) +  pd.DateOffset(days=1)).unique().tolist()\n",
    "# trandate_6mo_2d_list = (insiders_df['trandate'] + pd.DateOffset(months=6) +  pd.DateOffset(days=2)).unique().tolist()\n",
    "\n",
    "# # Combine all date lists\n",
    "# combined_date_list = list(set(trandate_list + trandate_6mo_list + trandate_6mo_1d_list + trandate_6mo_2d_list))\n",
    "# print(len(combined_date_list))\n",
    "# start_time = datetime.now()\n",
    "# print(start_time)\n",
    "# final_prices_df = pd.DataFrame()\n",
    "# date_chunks = list(chunks(combined_date_list, 3000))\n",
    "\n",
    "# for i, date_chunk in enumerate(date_chunks):\n",
    "#     db = wrds.Connection(wrds_username='asherbaraban')\n",
    "#     chunk_start_time = datetime.now()\n",
    "#     print(f\"Processing chunk {i+1}/{len(date_chunks)}\")\n",
    "\n",
    "#     chunk_prices_df = fetch_prices_for_dates(cusip_list, date_chunk)\n",
    "#     chunk_prices_df['datadate'] = pd.to_datetime(chunk_prices_df['datadate'])\n",
    "#     final_prices_df = pd.concat([final_prices_df, chunk_prices_df], ignore_index=True)\n",
    "\n",
    "#     chunk_end_time = datetime.now()\n",
    "#     print(f\"Chunk {i+1} processed in: {chunk_end_time - chunk_start_time}\")\n",
    "#     db.close()\n",
    "\n",
    "# end_time = datetime.now()\n",
    "# print(f\"Data fetched in: {end_time - start_time}\")\n",
    "\n",
    "# # Verify the concatenated DataFrame\n",
    "# print(f\"Total rows fetched: {len(final_prices_df)}\")\n",
    "# print(final_prices_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_prices_df.to_csv(\"raw_prices.csv\", index=False)\n",
    "# final_prices_df.to_pickle('raw_prices.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_prices_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prices_df = pd.read_pickle(\"raw_prices.pkl\")\n",
    "final_prices_df = downcast_numeric_columns(final_prices_df)\n",
    "insiders_df = downcast_numeric_columns(insiders_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insiders_df['trandate_6mo'] = insiders_df['trandate'] + pd.DateOffset(months=6)\n",
    "insiders_df['trandate_6mo_1'] = insiders_df['trandate'] + pd.DateOffset(months=6) +  pd.DateOffset(days=1)\n",
    "insiders_df['trandate_6mo_2'] = insiders_df['trandate'] + pd.DateOffset(months=6) +  pd.DateOffset(days=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insiders_df = insiders_df.merge(final_prices_df,\n",
    "                                left_on=['cusipi', 'trandate'],\n",
    "                                right_on=['cusip', 'datadate'],\n",
    "                                how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insiders_df = insiders_df.merge(final_prices_df,\n",
    "                                left_on=['cusipi', 'trandate_6mo'],\n",
    "                                right_on=['cusip', 'datadate'],\n",
    "                                how='left',\n",
    "                                suffixes=('', '_6mo'))\n",
    "insiders_df = insiders_df.merge(final_prices_df,\n",
    "                                left_on=['cusipi', 'trandate_6mo_1'],\n",
    "                                right_on=['cusip', 'datadate'],\n",
    "                                how='left',\n",
    "                                suffixes=('', '_6mo_1'))\n",
    "insiders_df = insiders_df.merge(final_prices_df,\n",
    "                                left_on=['cusipi', 'trandate_6mo_2'],\n",
    "                                right_on=['cusip', 'datadate'],\n",
    "                                how='left',\n",
    "                                suffixes=('', '_6mo_2'))\n",
    "\n",
    "insiders_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to check in order of priority\n",
    "price_columns = ['prccd_6mo', 'prccd_6mo_1', 'prccd_6mo_2']\n",
    "adjustment_columns = ['ajexdi_6mo', 'ajexdi_6mo_1', 'ajexdi_6mo_2']\n",
    "total_return_columns = ['trfd_6mo', 'trfd_6mo_1', 'trfd_6mo_2']\n",
    "\n",
    "# Apply the function to each row to get the first non-NA value\n",
    "insiders_df['prccd_6mo_consolidated'] = insiders_df.apply(lambda row: first_non_na(row, price_columns), axis=1)\n",
    "insiders_df['ajexdi_6mo_consolidated'] = insiders_df.apply(lambda row: first_non_na(row, adjustment_columns), axis=1)\n",
    "insiders_df['trfd_6mo_consolidated'] = insiders_df.apply(lambda row: first_non_na(row, total_return_columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insiders_df['prccd_adjusted_begin'] = insiders_df['prccd'] / insiders_df['ajexdi']\n",
    "insiders_df['prccd_adjusted_end'] = insiders_df['prccd_6mo_consolidated'] / insiders_df['ajexdi_6mo_consolidated']\n",
    "\n",
    "insiders_df['total_return_6mo'] = (\n",
    "    (insiders_df['prccd_adjusted_end'] * insiders_df['trfd_6mo_consolidated']) /\n",
    "    (insiders_df['prccd_adjusted_begin'] * insiders_df['trfd'])\n",
    ") - 1\n",
    "\n",
    "insiders_df.info()\n",
    "insiders_df.to_pickle(\"merged_prices_insiders.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insiders_df.to_pickle(\"merged_prices_insiders.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insiders_df = pd.read_pickle(\"merged_prices_insiders.pkl\")\n",
    "insiders_df['trandate'] = pd.to_datetime(insiders_df['trandate'])\n",
    "insiders_df['trandate_6mo'] = pd.to_datetime(insiders_df['trandate_6mo'])\n",
    "insiders_df['trandate_6mo_1'] = pd.to_datetime(insiders_df['trandate_6mo_1'])\n",
    "insiders_df['trandate_6mo_2'] = pd.to_datetime(insiders_df['trandate_6mo_2'])\n",
    "insiders_df = downcast_numeric_columns(insiders_df)\n",
    "\n",
    "object_cols = ['owner', 'rolecode1', 'rolecode2', 'rolecode3', 'rolecode4', 'cname', 'ticker', 'sector', 'ownership', 'cleanse', 'acqdisp', 'cusipi']\n",
    "for col in object_cols:\n",
    "    insiders_df[col] = insiders_df[col].astype('category')\n",
    "insiders_df['dcn'] = insiders_df['dcn'].astype('string')\n",
    "insiders_df['seqnum'] = pd.to_numeric(insiders_df['seqnum'], downcast='integer')\n",
    "insiders_df['personid'] = pd.to_numeric(insiders_df['personid'], downcast='integer')\n",
    "\n",
    "db = wrds.Connection(wrds_username='asherbaraban')\n",
    "sector_etfs = {\n",
    "    1: \"XLF\",  # Finance\n",
    "    2: \"XLV\",  # Healthcare\n",
    "    3: \"IYK\",  # Consumer Non-Durable\n",
    "    4: \"XLY\",  # Consumer Services\n",
    "    5: \"XHB\",  # Consumer Durables\n",
    "    6: \"XLE\",  # Energy\n",
    "    7: \"IYT\",  # Transportation\n",
    "    8: \"XLK\",  # Technology\n",
    "    9: \"XLB\",  # Basic Industries\n",
    "    10: \"XLI\",  # Capital Goods\n",
    "    11: \"XLU\",  # Public Utilities\n",
    "    99: \"VTI\",  # Miscellaneous\n",
    "    0: \"VTI\",  # Not Classified\n",
    "    \"XX\": \"VTI\",  # Not Classified\n",
    "}\n",
    "\n",
    "benchmark_tickers = sector_etfs.values()\n",
    "tickers_placeholder = ', '.join(f\"'{ticker}'\" for ticker in benchmark_tickers)\n",
    "\n",
    "query = f\"\"\"\n",
    "        SELECT \n",
    "            cusip,\n",
    "            datadate,\n",
    "            ajexdi,\n",
    "            trfd,\n",
    "            prccd,\n",
    "            tic\n",
    "        FROM \n",
    "            comp_na_daily_all.secd\n",
    "        WHERE \n",
    "            tic IN ({tickers_placeholder}) AND\n",
    "            datadate = '2023-10-02'\n",
    "    \"\"\"\n",
    "prices = db.raw_sql(query)\n",
    "\n",
    "ticker_to_cusip = prices.set_index('tic')['cusip'].to_dict()\n",
    "# Generate the additional dates\n",
    "trandate_list = insiders_df['trandate'].unique().tolist()\n",
    "trandate_6mo_list = (insiders_df['trandate'] + pd.DateOffset(months=6)).unique().tolist()\n",
    "trandate_6mo_1d_list = (insiders_df['trandate'] + pd.DateOffset(months=6) +  pd.DateOffset(days=1)).unique().tolist()\n",
    "trandate_6mo_2d_list = (insiders_df['trandate'] + pd.DateOffset(months=6) +  pd.DateOffset(days=2)).unique().tolist()\n",
    "\n",
    "# Combine all date lists\n",
    "combined_date_list = list(set(trandate_list + trandate_6mo_list + trandate_6mo_1d_list + trandate_6mo_2d_list))\n",
    "\n",
    "# Now query the right dates to get total returns for all of the right intervals \n",
    "etf_prices = fetch_prices_for_dates(ticker_to_cusip.values(), combined_date_list)\n",
    "etf_prices['datadate'] = pd.to_datetime(insiders_df['datadate'])\n",
    "insiders_df['sector_ticker'] = insiders_df['sector'].map(sector_etfs)\n",
    "insiders_df['sector_cusip'] = insiders_df['sector_ticker'].map(ticker_to_cusip)\n",
    "etf_prices['cusip'] = etf_prices['cusip'].astype('category')\n",
    "etf_prices.dropna(subset=['datadate'], inplace=True)\n",
    "\n",
    "etf_prices['price_adj'] = (etf_prices['prccd'] * etf_prices['trfd']) / etf_prices['ajexdi']\n",
    "etf_prices.drop(columns = ['prccd', 'trfd', 'ajexdi'], inplace=True)\n",
    "\n",
    "etf_prices.rename(columns={'datadate': 'date', 'cusip': 'sector_cusip'}, inplace=True)\n",
    "etf_prices.set_index(['date', 'sector_cusip'], inplace=True)\n",
    "etf_prices.sort_index(inplace=True)\n",
    "etf_prices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "insiders_df['sector_cusip'] = insiders_df['sector_cusip'].astype('category')\n",
    "insiders_df['sector_ticker'] = insiders_df['sector_ticker'].astype('category')\n",
    "insiders_df['industry'] = insiders_df['industry'].astype('category')\n",
    "insiders_df['cusip_6mo'] = insiders_df['cusip_6mo'].astype('string')\n",
    "\n",
    "substrings = ['trfd', 'ajexdi', 'prccd', 'datadate', 'cusip_', 'sector_ticker', 'industry', 'cname', 'tprice']\n",
    "\n",
    "# Identify columns to drop based on the substrings\n",
    "columns_to_drop = [col for col in insiders_df.columns if any(sub in col for sub in substrings)]\n",
    "\n",
    "# Drop the identified columns inplace\n",
    "insiders_df.drop(columns=columns_to_drop, inplace=True)\n",
    "insiders_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge with trandate_6mo\n",
    "insiders_df.rename(columns={'trandate': 'date'}, inplace=True)\n",
    "print(insiders_df.columns)\n",
    "insiders_df.set_index(['date', 'sector_cusip'], inplace=True)\n",
    "insiders_df = insiders_df.merge(etf_prices, left_index=True, right_index=True, how='left', suffixes=('', '_sec'))\n",
    "insiders_df.reset_index(inplace=True)\n",
    "insiders_df.rename(columns={'date': 'trandate'}, inplace=True)\n",
    "print(\"A completed\")\n",
    "\n",
    "# Merge with trandate_6mo\n",
    "insiders_df.rename(columns={'trandate_6mo': 'date'}, inplace=True)\n",
    "insiders_df.set_index(['date', 'sector_cusip'], inplace=True)\n",
    "insiders_df = insiders_df.merge(etf_prices, left_index=True, right_index=True, how='left', suffixes=('', '_sec_6mo'))\n",
    "insiders_df.reset_index(inplace=True)\n",
    "insiders_df.rename(columns={'date': 'trandate_6mo'}, inplace=True)\n",
    "print(\"B completed\")\n",
    "\n",
    "# Merge with trandate_6mo_1\n",
    "insiders_df.rename(columns={'trandate_6mo_1': 'date'}, inplace=True)\n",
    "insiders_df.set_index(['date', 'sector_cusip'], inplace=True)\n",
    "insiders_df = insiders_df.merge(etf_prices, left_index=True, right_index=True, how='left', suffixes=('', '_sec_6mo_1'))\n",
    "insiders_df.reset_index(inplace=True)\n",
    "insiders_df.rename(columns={'date': 'trandate_6mo_1'}, inplace=True)\n",
    "print(\"C completed\")\n",
    "\n",
    "# Merge with trandate_6mo_2\n",
    "insiders_df.rename(columns={'trandate_6mo_2': 'date'}, inplace=True)\n",
    "insiders_df.set_index(['date', 'sector_cusip'], inplace=True)\n",
    "insiders_df = insiders_df.merge(etf_prices, left_index=True, right_index=True, how='left', suffixes=('', '_sec_6mo_2'))\n",
    "insiders_df.reset_index(inplace=True)\n",
    "insiders_df.rename(columns={'date': 'trandate_6mo_2'}, inplace=True)\n",
    "print(\"All merges completed\")\n",
    "\n",
    "# Save the merged dataframe\n",
    "insiders_df.to_pickle(\"post_sector.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insiders_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Assuming insiders_df is already defined and has the column 'trandate'\n",
    "insiders_df['trandate_year'] = insiders_df['trandate'].dt.year\n",
    "\n",
    "# Group by the extracted year and calculate the mean total return\n",
    "annual_return = insiders_df.groupby(['trandate_year', 'acqdisp'])['total_return_6mo'].mean().reset_index()\n",
    "annual_return = annual_return[annual_return['acqdisp'] == 'A']\n",
    "# Create a line plot of the average total return by year\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(annual_return['trandate_year'], annual_return['total_return_6mo'], marker='o', linestyle='-')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Total Return (6 Months)')\n",
    "plt.title('Average 6-Month Total Return by Year')\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
